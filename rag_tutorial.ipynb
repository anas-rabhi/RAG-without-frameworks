{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "The only libraries that are going to be used are : \n",
    "- OpenAI : To call the LLM & Embedding model\n",
    "- PyPDF2 : To process the text inside each PDF\n",
    "- ChromaDB : To create a VectorDB and save the documents/chunks and their embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set up OpenAI API (make sure to set your API key as an environment variable)\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-....\"\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Chroma Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Chroma client : Persistent client to save this DB into the Disk. \n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "# Create a collection \n",
    "collection = chroma_client.create_collection(\"pdf_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "The idea here is :\n",
    "- Retrieve all the text from PDFs\n",
    "- Divide them into chunks\n",
    "- Get for each chunk a vector representation (embeddings)\n",
    "- Store each chunk inside the ChromaDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract all the text from a PDF file.\n",
    "    \"\"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PdfReader(file)\n",
    "        text = \"\"\n",
    "        # Iterate through all pages and extract text\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def split_text_into_chunks(text, words_per_chunk=500, overlap=50):\n",
    "    \"\"\"\n",
    "    Split text into smaller chunks for better embedding.\n",
    "    \"\"\"\n",
    "    # Split text into words\n",
    "    words = re.findall(r'\\S+', text)\n",
    "    chunks = []\n",
    "    # Create overlapping chunks\n",
    "    for i in range(0, len(words), words_per_chunk - overlap):\n",
    "        chunk = ' '.join(words[i:i + words_per_chunk])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"\n",
    "    Call OpenAI API to create embeddings for a given text.\n",
    "    \"\"\"\n",
    "    # Call OpenAI API to generate embedding\n",
    "\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "def process_pdfs(folder_path):\n",
    "    \"\"\"\n",
    "    Process the PDFs: \n",
    "    1. Extract the text\n",
    "    2. Convert the text into chunks\n",
    "    3. Get embeddings for each chunk\n",
    "    4. Load the chunks and the embeddings inside the chroma DB\n",
    "    \n",
    "    PS : We can also request OpenAI with batches...\n",
    "    \"\"\"\n",
    "\n",
    "    doc_id = 0\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            print('file: ', filename)\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # Extract text from PDF\n",
    "            print('Extract text from pdf')\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "            \n",
    "            # Split text into chunks\n",
    "            print('Split text')\n",
    "            chunks = split_text_into_chunks(text)\n",
    "            print('Chunks size : ', len(chunks))\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                # Create embeddings using OpenAI\n",
    "                embedding = get_embedding(chunk)\n",
    "                \n",
    "                # Add to Chroma: Some of the VectorDB allow inserting batches\n",
    "                collection.add(\n",
    "                    embeddings=[embedding],\n",
    "                    documents=[chunk],\n",
    "                    metadatas=[{\"source\": filename, \"chunk\": i}],\n",
    "                    ids=[f\"{filename}_chunk_{i}\"]\n",
    "                )\n",
    "                doc_id += 1\n",
    "\n",
    "    print(f\"Processed and added {len(collection.get()['ids'])} chunks to Chroma.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:  31555_1a_maj1_diagnostic.pdf\n",
      "Extract text from pdf\n",
      "Split text\n",
      "Chunks size :  182\n",
      "file:  at105_avril24_0.pdf\n",
      "Extract text from pdf\n",
      "Split text\n",
      "Chunks size :  21\n",
      "file:  a_toulouse_mars_2024.pdf\n",
      "Extract text from pdf\n",
      "Split text\n",
      "Chunks size :  15\n",
      "file:  livret_code_de_la_rue_170x230-v2.pdf\n",
      "Extract text from pdf\n",
      "Split text\n",
      "Chunks size :  9\n",
      "Processed and added 227 chunks to Chroma.\n"
     ]
    }
   ],
   "source": [
    "data_folder = \"./data\"  # Replace it with your PDF folder path\n",
    "process_pdfs(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble everything\n",
    "\n",
    "The process, once the ChromaDB is populated, for each user question:\n",
    "- Generate the embedding of the question\n",
    "- Query ChromaDB with the question embedding to retrieve the most relevant chunks\n",
    "- Send the retrieved chunks and the original question to the LLM to formulate an answer based on the provided context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chroma(query_embedding, n_results=5):\n",
    "    \"\"\"\n",
    "    Query ChromaDB for relevant documents using the query embedding.\n",
    "    \"\"\"\n",
    "    # Query ChromaDB\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    # Return the first list of documents\n",
    "    return results['documents'][0]\n",
    "\n",
    "def generate_answer(question, context):\n",
    "    \"\"\"\n",
    "    Generate an answer using OpenAI's API with the given context.\n",
    "    \"\"\"\n",
    "    # Call OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"You are a helpful assistant. Use the following context to answer the user's question: {context}\\n\\n --------\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "    )\n",
    "    # Extract and return the generated answer\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(question):\n",
    "    \"\"\"\n",
    "    Executes the RAG pipeline.\n",
    "    \"\"\"\n",
    "    # Generate embedding for the question\n",
    "    question_embeddings = get_embedding(question)\n",
    "\n",
    "    # Retrieve relevant chunks from ChromaDB\n",
    "    relevant_chunks = query_chroma(question_embeddings)\n",
    "\n",
    "    # Combine chunks into a single context string\n",
    "    context = \"\\n\\n\".join(relevant_chunks)\n",
    "\n",
    "    # Generate answer using the question and context\n",
    "    answer = generate_answer(question, context)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To obtain a free composteur (composter) in Toulouse, you need to follow these steps:\n",
      "\n",
      "1. **Complete the Biodéchets Sorting Training:**\n",
      "   - The primary condition for receiving a free composter is to complete a training session on sorting biodéchets (biodegradable waste) provided by Toulouse Métropole.\n",
      "\n",
      "2. **Attend the Training Based on Your Situation:**\n",
      "   - If you have a garden and live in a house or a ground-floor apartment with a garden:\n",
      "     - Participate in an individual training session that lasts half a day to learn how to compost in your garden.\n",
      "   - If you live near a public garden and want to start a collective composting project:\n",
      "     - Fill out a form to assess the feasibility of your project.\n",
      "     - Attend a training session that lasts two days and a half-day to become equipped to help families sort biodéchets.\n",
      "   - If you have encouraged your neighbors in your residence to collectively sort biodéchets:\n",
      "     - Gather 10 or more families in your residence for the initiative.\n",
      "     - Appoint a person who will undergo a one-day training session on biodéchets sorting.\n",
      "     - This person will then guide and support the entire group in sorting biodéchets.\n",
      "\n",
      "3. **Reserve Your Composter:**\n",
      "   - You need to reserve your composter by visiting the website www.metropole.toulouse.fr.\n",
      "   - Click on “Mes démarches” (My procedures) to proceed with the reservation.\n",
      "\n",
      "4. **Collect Your Composter:**\n",
      "   - Use the QR code provided in the informational documents or the website to facilitate the collection process of your free composter.\n",
      "\n",
      "By following these steps and attending the necessary training, you can successfully receive a free composter from Toulouse Métropole. For more details and to participate, you can visit the website www.jeparticipe.metropole.toulouse.fr.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How to get a composteur gratuit\"\n",
    "r = rag_pipeline(prompt)\n",
    "print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
